{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson-01 Assignment\n",
    "\n",
    "各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. 复现课堂代码\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。\n",
    "\n",
    "- 2. 请回答以下问题\n",
    "回答以下问题，并将问题发送至 mqgao@kaikeba.com中：\n",
    "\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "- 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(所以请务必把GitHub按照班主任要求录入在Trello中)； 第2问，请提交至mqgao@kaikeba.com邮箱。\n",
    "\n",
    "- 4. 作业截止时间\n",
    "此次作业截止时间为 2019.7.6日\n",
    "\n",
    "- 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基础理论部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0. Can you come up out 3 sceneraies which use AI methods?\n",
    "Ans:  \n",
    "medical  drive-self  education\n",
    "1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "Ans:  \n",
    "    step 1 Create a Repository\n",
    "    step 2 Create a Brance \n",
    "    step 3 Make and commit changesm\n",
    "    step 4 open and Pull Request\n",
    "    setp 5 Merge your Pull Request\n",
    "    \n",
    "    \n",
    "\n",
    "2. What's the Probability Model?\n",
    "Ans:\n",
    "\n",
    "3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "Ans:\n",
    "\n",
    "4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "Ans:\n",
    "\n",
    "5. What's the Language Model;\n",
    "Ans:\n",
    "\n",
    "6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "Ans:\n",
    "\n",
    "7. What's the 1-gram language model;\n",
    "Ans: 计算一个词出现的概率。\n",
    "\n",
    "8. What's the disadvantages and advantages of 1-gram language model;\n",
    "Ans:\n",
    "\n",
    "9. What't the 2-gram models;\n",
    "Ans: 计算两个词出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。\n",
    "\n",
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    " WstWorld\n",
    "\n",
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "\n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "一个“接待员”的语言可以定义为\n",
    "\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个客户在4S店售车现场，咨询汽车信息的场景可以定义为：\n",
    "human = \"\"\"\n",
    "human = 客户 咨询 车辆信息\n",
    "客户 = 我|俺|我们 \n",
    "咨询 = 请教您|问下工程师\n",
    "车辆信息 = 关于D60车子的上市时间 | 这个车子百公里的油耗 \n",
    "\"\"\"\n",
    "#一个“智能机器人”的语言可以定义为\n",
    "\n",
    "bot = \"\"\"\n",
    "bot = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号销售经理 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 \n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 =了解关于车子哪些信息|想咨询车子哪方面的内容呢 \n",
    "车辆信息 = 价格 | 颜色 | 油耗 | 上市时间\n",
    "结尾 =？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(grammar_str,split=\"=>\",line_split=\"\\n\"):\n",
    "    grammar ={}\n",
    "    for line in  grammar_str.split(line_split):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        exp,stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split(\"|\")]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.create_grammar(grammar_str, split='=>', line_split='\\n')>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = random.choice\n",
    "def generate (gram,target):\n",
    "    if target not in gram: return target\n",
    "    expanded = [generate(gram,t) for t in choice(gram[target])]\n",
    "    return \"\".join([e for e in expanded if e != \"null\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们请教您这个车子百公里的油耗\n",
      "我们请教您这个车子百公里的油耗\n",
      "我问下工程师这个车子百公里的油耗\n",
      "俺问下工程师这个车子百公里的油耗\n",
      "我们问下工程师这个车子百公里的油耗\n",
      "我们问下工程师关于D60车子的上市时间\n",
      "我们问下工程师这个车子百公里的油耗\n",
      "俺请教您这个车子百公里的油耗\n",
      "我们请教您这个车子百公里的油耗\n",
      "我们请教您关于D60车子的上市时间\n",
      "俺问下工程师这个车子百公里的油耗\n",
      "我们请教您关于D60车子的上市时间\n",
      "我问下工程师这个车子百公里的油耗\n",
      "我请教您关于D60车子的上市时间\n",
      "俺问下工程师这个车子百公里的油耗\n",
      "俺请教您关于D60车子的上市时间\n",
      "我们请教您关于D60车子的上市时间\n",
      "我们请教您关于D60车子的上市时间\n",
      "我们请教您这个车子百公里的油耗\n",
      "俺问下工程师这个车子百公里的油耗\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generate(gram = create_grammar(human,split=\"=\"),target=\"human\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "按照我们上文中定义的prob_2函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "修改代码，获得新的2-gram语言模型\n",
    "进行文本清洗，获得所有的纯文本\n",
    "将这些文本进行切词\n",
    "送入之前定义的语言模型中，判断文本的合理程度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成n个句子，并能选择一个最合理的句子:\n",
    "\n",
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个计算概率的函数：\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2: return words_count_2[word1+word2] / len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再定义一个计算一句话出现的可能性：\n",
    "def get_probability(sentences):\n",
    "    prob = []\n",
    "    for sen in sentences:\n",
    "        words = cut(sen)\n",
    "        sentence_prob = 1\n",
    "\n",
    "        for i, word in enumerate(words[:-1]):\n",
    "            next_ = words[i+1]\n",
    "            probability = prob_2(word, next_)\n",
    "            sentence_prob *= probability\n",
    "        prob.append(sentence_prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最后，把所有函数放在一起，成为一个能够找出的最有可能的句子的函数：\n",
    "def generate_best(grammer_str, target, n, line_split=\"\\n\"):\n",
    "    example_grammer = create_grammer(grammer_str)\n",
    "    sentence_n = generate_n(gram=example_grammer, target=target,n=n)\n",
    "    file = \"....csv\" # 这里隐去了语料库源\n",
    "    content = pd.read_csv(file)\n",
    "    comment = content[\"comment\"]\n",
    "    comment_clean = [\"\".join(token(str(a))) for a in comment]\n",
    "\n",
    "    TOKEN = []\n",
    "    for i, line in enumerate(comment_clean):\n",
    "        TOKEN += cut(line)\n",
    "\n",
    "    words_count = Counter(TOKEN)\n",
    "\n",
    "    TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-1]))]\n",
    "    words_count_2 = Counter(TOKEN_2_GRAM)\n",
    "\n",
    "    prob = get_probability(sentence_n)\n",
    "    sen_prob = zip(sentence_n, prob)\n",
    "    prob = sorted(sen_prob, key=lambda x: x[1], reverse=True)\n",
    "    print(f\"The most likely sentence is '{prob[0][0]}' with probability {prob[0][1]} among {n} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们使用一个新的规则和语料库训练模型：\n",
    "grammer_1 = \"\"\"\n",
    "sentence = 主语结构 谓语结构 宾语结构\n",
    "主语结构 = 定语 主语 | 主语\n",
    "谓语结构 = 状语 谓语 | 谓语\n",
    "宾语结构 = 定语 宾语 | 宾语\n",
    "定语 = 高大的 | 矮小的 | 巨大的 | 渺小的 | 漂亮的 | 丑陋的 | 红色的 | 饥饿的 | 饱满的 | 可怜的 | 快乐的 | 补水的 | 无奈的\n",
    "主语 = 我 | 你 | 他 | 她 | 它 | 我们 | 你们 | 他们\n",
    "状语 = 狠狠地 | 高兴地 | 快速地 | 奇怪地 | 绝对地\n",
    "谓语 = 跑 | 跳 | 玩耍 | 打 | 打扮 | 践踏 | 鄙视 | 尊重 | 勾引\n",
    "宾语 = 小狗 | 女孩 | 工人 | 足球 | 电脑 | 手机 | 电话 | 北京 | 大海\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_grammer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-6dc54bf41b39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_best\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammer_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrammer_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sentence\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-7e5c35991daa>\u001b[0m in \u001b[0;36mgenerate_best\u001b[1;34m(grammer_str, target, n, line_split)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#最后，把所有函数放在一起，成为一个能够找出的最有可能的句子的函数：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_best\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammer_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mexample_grammer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_grammer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammer_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msentence_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexample_grammer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"....csv\"\u001b[0m \u001b[1;31m# 这里隐去了语料库源\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_grammer' is not defined"
     ]
    }
   ],
   "source": [
    " generate_best(grammer_str=grammer_1, target=\"sentence\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (Optional) 完成基于Pattern Match的语句问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (Optional) 完成阿兰图灵机器智能原始论文的阅读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf\n",
    "并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
